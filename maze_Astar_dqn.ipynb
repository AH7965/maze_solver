{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Maze with A-star algorithm, Q-learning and Deep Q-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of this notebook is to solve self-made maze with A-star algorithm, Q-learning and Deep Q-network.\n",
    "### The maze is in square shape, consists of start point, goal point and tiles in the mid of them.\n",
    "### Each tile has numericals as its point. In other words, if you step on to the tile with -1, you get 1 point subtracted.\n",
    "### The maze has blocks to prevent you from taking the route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pds\n",
    "import random\n",
    "import copy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from collections import deque\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    def __init__(self, size=10, blocks_rate=0.1):\n",
    "        self.size = size if size > 3 else 10\n",
    "        self.blocks = int((size ** 2) * blocks_rate) \n",
    "        self.s_list = []\n",
    "        self.maze_list = []\n",
    "        self.e_list = []\n",
    "\n",
    "    def create_mid_lines(self, k):\n",
    "        if k == 0: self.maze_list.append(self.s_list)\n",
    "        elif k == self.size - 1: self.maze_list.append(self.e_list)\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            for l in range(0,self.size):\n",
    "                if l == 0: tmp_list.extend(\"#\")\n",
    "                elif l == self.size-1: tmp_list.extend(\"#\")\n",
    "                else:\n",
    "                    a = random.randint(-1, 0)\n",
    "                    tmp_list.extend([a])\n",
    "            self.maze_list.append(tmp_list)\n",
    "\n",
    "    def insert_blocks(self, k, s_r, e_r):\n",
    "        b_y = random.randint(1, self.size-2)\n",
    "        b_x = random.randint(1, self.size-2)\n",
    "        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1\n",
    "        else: self.maze_list[b_y][b_x] = \"#\"\n",
    "            \n",
    "    def generate_maze(self): \n",
    "        s_r = random.randint(1, (self.size / 2) - 1)\n",
    "        for i in range(0, self.size):\n",
    "            if i == s_r: self.s_list.extend(\"S\")\n",
    "            else: self.s_list.extend(\"#\")\n",
    "        start_point = [0, s_r]\n",
    "\n",
    "        e_r = random.randint((self.size / 2) + 1, self.size - 2)\n",
    "        for j in range(0, self.size):\n",
    "            if j == e_r: self.e_list.extend([50])\n",
    "            else: self.e_list.extend(\"#\")\n",
    "        goal_point = [self.size - 1, e_r]\n",
    "\n",
    "        for k in range(0, self.size):\n",
    "            self.create_mid_lines(k)\n",
    "        \n",
    "        for k in range(self.blocks):\n",
    "            self.insert_blocks(k, s_r, e_r)\n",
    "\n",
    "        return self.maze_list, start_point, goal_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Field(object):\n",
    "    def __init__(self, maze, start_point, goal_point):\n",
    "        self.maze = maze\n",
    "        self.start_point = start_point\n",
    "        self.goal_point = goal_point\n",
    "        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    def display(self, point=None):\n",
    "        field_data = copy.deepcopy(self.maze)\n",
    "        if not point is None:\n",
    "                y, x = point\n",
    "                field_data[y][x] = \"@@\"\n",
    "        else:\n",
    "                point = \"\"\n",
    "        for line in field_data:\n",
    "                print (\"\\t\" + \"%3s \" * len(line) % tuple(line))\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        movables = []\n",
    "        if state == self.start_point:\n",
    "            y = state[0] + 1\n",
    "            x = state[1]\n",
    "            a = [[y, x]]\n",
    "            return a\n",
    "        else:\n",
    "            for v in self.movable_vec:\n",
    "                y = state[0] + v[0]\n",
    "                x = state[1] + v[1]\n",
    "                if not(0 < x < len(self.maze) and\n",
    "                       0 <= y <= len(self.maze) - 1 and\n",
    "                       maze[y][x] != \"#\" and\n",
    "                       maze[y][x] != \"S\"):\n",
    "                    continue\n",
    "                movables.append([y,x])\n",
    "            if len(movables) != 0:\n",
    "                return movables\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def get_val(self, state):\n",
    "        y, x = state\n",
    "        if state == self.start_point: return 0, False\n",
    "        else:\n",
    "            v = float(self.maze[y][x])\n",
    "            if state == self.goal_point: \n",
    "                return v, True\n",
    "            else: \n",
    "                return v, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n"
     ]
    }
   ],
   "source": [
    "size = 10\n",
    "barriar_rate = 0.1\n",
    "\n",
    "maze_1 = Maze(size, barriar_rate)\n",
    "maze, start_point, goal_point = maze_1.generate_maze()\n",
    "maze_field = Field(maze, start_point, goal_point)\n",
    "\n",
    "maze_field.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the maze with A-star algorithm\n",
    "### https://en.wikipedia.org/wiki/A*_search_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node(object):    \n",
    "    def __init__(self, state, start_point, goal_point):\n",
    "        self.state = state\n",
    "        self.start_point = start_point\n",
    "        self.goal_point = goal_point\n",
    "        self.hs = (self.state[0] - self.goal_point[0]) ** 2 + (self.state[1] - self.goal_point[1]) ** 2\n",
    "        self.fs = 0\n",
    "        self.parent_node = None\n",
    "    \n",
    "    def confirm_goal(self):\n",
    "        if self.goal_point == self.state: return True\n",
    "        else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NodeList(list):\n",
    "    def find_nodelist(self, state):\n",
    "        node_list = [t for t in self if t.state==state]\n",
    "        return node_list[0] if node_list != [] else None\n",
    "    def remove_from_nodelist(self, node):\n",
    "        del self[self.index(node)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Aster_Solver(object):\n",
    "    def __init__(self, maze, start_point, goal_point, display=False):\n",
    "        self.Field = maze\n",
    "        self.start_point = start_point\n",
    "        self.goal_point = goal_point\n",
    "        self.open_list = NodeList()\n",
    "        self.close_list = NodeList()\n",
    "        self.steps = 0\n",
    "        self.score = 0\n",
    "        self.display = display\n",
    "        \n",
    "    def set_initial_node(self):\n",
    "        node = Node(self.start_point, self.start_point, self.goal_point)\n",
    "        node.start_point = self.start_point\n",
    "        node.goal_point = self.goal_point \n",
    "        return node\n",
    "                \n",
    "    def go_next(self, next_actions, node):\n",
    "        node_gs = node.fs - node.hs\n",
    "        for action in next_actions:\n",
    "            open_list = self.open_list.find_nodelist(action)\n",
    "            dist = (node.state[0] - action[0]) ** 2 + (node.state[1] - action[1]) ** 2\n",
    "            if open_list:\n",
    "                if open_list.fs > node_gs + open_list.hs + dist:\n",
    "                    open_list.fs = node_gs + open_list.hs + dist\n",
    "                    open_list.parent_node = node\n",
    "            else:\n",
    "                open_list = self.close_list.find_nodelist(action)\n",
    "                if open_list:\n",
    "                    if open_list.fs > node_gs + open_list.hs + dist:\n",
    "                        open_list.fs = node_gs + open_list.hs + dist\n",
    "                        open_list.parent_node = node\n",
    "                        self.open_list.append(open_list)\n",
    "                        self.close_list.remove_from_nodelist(open_list)\n",
    "                else:\n",
    "                    open_list = Node(action, self.start_point, self.goal_point)\n",
    "                    open_list.fs = node_gs + open_list.hs + dist\n",
    "                    open_list.parent_node = node\n",
    "                    self.open_list.append(open_list)\n",
    "    \n",
    "    def solve_maze(self):\n",
    "        node = self.set_initial_node()\n",
    "        node.fs = node.hs\n",
    "        self.open_list.append(node)\n",
    "        \n",
    "        while True:            \n",
    "            node = min(self.open_list, key = lambda node:node.fs)\n",
    "            print (\"current state:  {0}\".format(node.state))\n",
    "            \n",
    "            if self.display:\n",
    "                self.Field.display(node.state)\n",
    "            \n",
    "            reward, tf = self.Field.get_val(node.state)\n",
    "            self.score =  self.score + reward\n",
    "            print(\"current step: {0} \\t score: {1} \\n\".format(self.steps, self.score))\n",
    "            self.steps += 1\n",
    "            if tf == True:\n",
    "                print(\"Goal!\")\n",
    "                break\n",
    "\n",
    "            self.open_list.remove_from_nodelist(node)\n",
    "            self.close_list.append(node)\n",
    "            \n",
    "            next_actions = self.Field.get_actions(node.state)   \n",
    "            self.go_next(next_actions, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state:  [0, 1]\n",
      "\t  #  @@   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 0 \t score: 0 \n",
      "\n",
      "current state:  [1, 1]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  @@   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 1 \t score: -1.0 \n",
      "\n",
      "current state:  [2, 1]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  @@   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 2 \t score: -2.0 \n",
      "\n",
      "current state:  [3, 1]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #  @@   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 3 \t score: -2.0 \n",
      "\n",
      "current state:  [4, 1]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #  @@   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 4 \t score: -2.0 \n",
      "\n",
      "current state:  [4, 2]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0  @@   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 5 \t score: -2.0 \n",
      "\n",
      "current state:  [5, 2]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #  @@   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 6 \t score: -2.0 \n",
      "\n",
      "current state:  [6, 2]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1  @@  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 7 \t score: -2.0 \n",
      "\n",
      "current state:  [6, 3]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  @@   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 8 \t score: -3.0 \n",
      "\n",
      "current state:  [6, 4]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1  @@   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 9 \t score: -3.0 \n",
      "\n",
      "current state:  [7, 4]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  @@  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 10 \t score: -4.0 \n",
      "\n",
      "current state:  [7, 5]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  @@   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 11 \t score: -5.0 \n",
      "\n",
      "current state:  [7, 6]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1  @@  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 12 \t score: -5.0 \n",
      "\n",
      "current state:  [8, 6]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #  @@  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 13 \t score: -5.0 \n",
      "\n",
      "current state:  [8, 7]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  @@   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 14 \t score: -6.0 \n",
      "\n",
      "current state:  [9, 7]\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  @@   #   # \n",
      "current step: 15 \t score: 44.0 \n",
      "\n",
      "Goal!\n"
     ]
    }
   ],
   "source": [
    "astar_Solver = Aster_Solver(maze_field, start_point, goal_point, display=True)\n",
    "astar_Solver.solve_maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Solving the maze in Q-learning\n",
    "### https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearning_Solver(object):\n",
    "    def __init__(self, maze, display=False):\n",
    "        self.Qvalue = {}\n",
    "        self.Field = maze\n",
    "        self.alpha = 0.2\n",
    "        self.gamma  = 0.9\n",
    "        self.epsilon = 0.2\n",
    "        self.steps = 0\n",
    "        self.score = 0\n",
    "        self.display = display\n",
    "\n",
    "    def qlearn(self, greedy_flg=False):\n",
    "        state = self.Field.start_point\n",
    "        while True:\n",
    "            if greedy_flg:\n",
    "                self.steps += 1\n",
    "                action = self.choose_action_greedy(state)\n",
    "                print(\"current state: {0} -> action: {1} \".format(state, action))\n",
    "                if self.display:\n",
    "                    self.Field.display(action)\n",
    "                reward, tf = self.Field.get_val(action)\n",
    "                self.score =  self.score + reward\n",
    "                print(\"current step: {0} \\t score: {1}\\n\".format(self.steps, self.score))\n",
    "                if tf == True:\n",
    "                    print(\"Goal!\")\n",
    "                    break\n",
    "            else:\n",
    "                action = self.choose_action(state)    \n",
    "            if self.update_Qvalue(state, action):\n",
    "                break\n",
    "            else:\n",
    "                state = action\n",
    "\n",
    "    def update_Qvalue(self, state, action):\n",
    "        Q_s_a = self.get_Qvalue(state, action)\n",
    "        mQ_s_a = max([self.get_Qvalue(action, n_action) for n_action in self.Field.get_actions(action)])\n",
    "        r_s_a, finish_flg = self.Field.get_val(action)\n",
    "        q_value = Q_s_a + self.alpha * ( r_s_a +  self.gamma * mQ_s_a - Q_s_a)\n",
    "        self.set_Qvalue(state, action, q_value)\n",
    "        return finish_flg\n",
    "\n",
    "\n",
    "    def get_Qvalue(self, state, action):\n",
    "        state = (state[0],state[1])\n",
    "        action = (action[0],action[1])\n",
    "        try:\n",
    "            return self.Qvalue[state][action]\n",
    "        except KeyError:\n",
    "            return 0.0\n",
    "\n",
    "    def set_Qvalue(self, state, action, q_value):\n",
    "        state = (state[0],state[1])\n",
    "        action = (action[0],action[1])\n",
    "        self.Qvalue.setdefault(state,{})\n",
    "        self.Qvalue[state][action] = q_value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if self.epsilon < random.random():\n",
    "            return random.choice(self.Field.get_actions(state))\n",
    "        else:\n",
    "            return self.choose_action_greedy(state)\n",
    "\n",
    "    def choose_action_greedy(self, state):\n",
    "        best_actions = []\n",
    "        max_q_value = -100\n",
    "        for a in self.Field.get_actions(state):\n",
    "            q_value = self.get_Qvalue(state, a)\n",
    "            if q_value > max_q_value:\n",
    "                best_actions = [a,]\n",
    "                max_q_value = q_value\n",
    "            elif q_value == max_q_value:\n",
    "                best_actions.append(a)\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def dump_Qvalue(self):\n",
    "        print(\"##### Dump Qvalue #####\")\n",
    "        for i, s in enumerate(self.Qvalue.keys()):\n",
    "            for a in self.Qvalue[s].keys():\n",
    "                print(\"\\t\\tQ(s, a): Q(%s, %s): %s\" % (str(s), str(a), str(self.Qvalue[s][a])))\n",
    "            if i != len(self.Qvalue.keys())-1: \n",
    "                print('\\t------------------state   action   reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Dump Qvalue #####\n",
      "\t\tQ(s, a): Q((7, 3), (6, 3)): 22.383403999999945\n",
      "\t\tQ(s, a): Q((7, 3), (7, 4)): 26.96839999999995\n",
      "\t\tQ(s, a): Q((7, 3), (8, 3)): 20.944403999999945\n",
      "\t\tQ(s, a): Q((7, 3), (7, 2)): 19.944403999999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 7), (3, 7)): 19.316063599999943\n",
      "\t\tQ(s, a): Q((4, 7), (5, 7)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((4, 7), (4, 6)): 24.081559999999943\n",
      "\t\tQ(s, a): Q((4, 7), (4, 8)): 23.542559999999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 7), (5, 6)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((5, 7), (4, 7)): 22.573403999999943\n",
      "\t\tQ(s, a): Q((5, 7), (5, 8)): 26.158399999999947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 6), (5, 6)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((6, 6), (7, 6)): 35.63999999999995\n",
      "\t\tQ(s, a): Q((6, 6), (6, 5)): 28.868399999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 6), (5, 5)): 25.981559999999945\n",
      "\t\tQ(s, a): Q((5, 6), (5, 7)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((5, 6), (4, 6)): 24.081559999999943\n",
      "\t\tQ(s, a): Q((5, 6), (6, 6)): 32.075999999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 8), (3, 8)): 21.188303998670477\n",
      "\t\tQ(s, a): Q((2, 8), (2, 7)): 17.384218785523064\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 7), (7, 6)): 35.63999999999995\n",
      "\t\tQ(s, a): Q((7, 7), (7, 8)): 34.63999999999995\n",
      "\t\tQ(s, a): Q((7, 7), (8, 7)): 43.99999999999997\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 1), (3, 1)): 13.807666227959947\n",
      "\t\tQ(s, a): Q((2, 1), (1, 1)): 10.184209644647552\n",
      "\t\tQ(s, a): Q((2, 1), (2, 2)): 13.807666227959947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 2), (6, 3)): 22.383403999999945\n",
      "\t\tQ(s, a): Q((6, 2), (6, 1)): 17.13055723999994\n",
      "\t\tQ(s, a): Q((6, 2), (5, 2)): 18.13055723999994\n",
      "\t\tQ(s, a): Q((6, 2), (7, 2)): 19.944403999999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 6), (1, 5)): 14.260905638468861\n",
      "\t\tQ(s, a): Q((1, 6), (2, 6)): 17.60606358901876\n",
      "\t\tQ(s, a): Q((1, 6), (1, 7)): 15.645963608063878\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 7), (3, 8)): 21.188303999848387\n",
      "\t\tQ(s, a): Q((3, 7), (2, 7)): 17.384457239573162\n",
      "\t\tQ(s, a): Q((3, 7), (4, 7)): 22.573403999999943\n",
      "\t\tQ(s, a): Q((3, 7), (3, 6)): 20.67340399993786\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 5), (1, 5)): 14.26091151488946\n",
      "\t\tQ(s, a): Q((2, 5), (2, 6)): 17.606063599999942\n",
      "\t\tQ(s, a): Q((2, 5), (2, 4)): 16.0465015159999\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 2), (7, 3)): 23.271559999999948\n",
      "\t\tQ(s, a): Q((7, 2), (6, 2)): 20.145063599999943\n",
      "\t\tQ(s, a): Q((7, 2), (8, 2)): 17.849963599999946\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 2), (1, 1)): 10.184209644647552\n",
      "\t\tQ(s, a): Q((1, 2), (2, 2)): 13.807666227959947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 8), (2, 8)): 18.06947340293805\n",
      "\t\tQ(s, a): Q((3, 8), (3, 7)): 19.316063597150947\n",
      "\t\tQ(s, a): Q((3, 8), (4, 8)): 23.542559999999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 5), (4, 5)): 22.383403999999945\n",
      "\t\tQ(s, a): Q((5, 5), (5, 6)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((5, 5), (5, 4)): 23.383403999999945\n",
      "\t\tQ(s, a): Q((5, 5), (6, 5)): 28.868399999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 1), (8, 2)): 17.849963599999946\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 6), (8, 6)): 39.59999999999996\n",
      "\t\tQ(s, a): Q((7, 6), (7, 5)): 31.07599999999995\n",
      "\t\tQ(s, a): Q((7, 6), (7, 7)): 38.59999999999996\n",
      "\t\tQ(s, a): Q((7, 6), (6, 6)): 32.075999999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 8), (3, 8)): 21.188303999999913\n",
      "\t\tQ(s, a): Q((4, 8), (4, 7)): 22.57340399999993\n",
      "\t\tQ(s, a): Q((4, 8), (5, 8)): 26.158399999999947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 4), (4, 5)): 22.383403999999945\n",
      "\t\tQ(s, a): Q((4, 4), (5, 4)): 23.383403999999945\n",
      "\t\tQ(s, a): Q((4, 4), (3, 4)): 18.940557239999944\n",
      "\t\tQ(s, a): Q((4, 4), (4, 3)): 18.940557239999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 3), (6, 4)): 25.981559999999945\n",
      "\t\tQ(s, a): Q((6, 3), (6, 2)): 20.145063599999943\n",
      "\t\tQ(s, a): Q((6, 3), (7, 3)): 23.271559999999948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 5), (2, 5)): 15.84545723996529\n",
      "\t\tQ(s, a): Q((1, 5), (1, 6)): 14.845456821575093\n",
      "\t\tQ(s, a): Q((1, 5), (1, 4)): 14.441851364276374\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 8), (5, 7)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((5, 8), (6, 8)): 30.17599999999995\n",
      "\t\tQ(s, a): Q((5, 8), (4, 8)): 23.542559999999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 6), (3, 7)): 19.31606359999994\n",
      "\t\tQ(s, a): Q((3, 6), (2, 6)): 17.606063599999754\n",
      "\t\tQ(s, a): Q((3, 6), (4, 6)): 24.081559999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 2), (1, 2)): 12.42689960516395\n",
      "\t\tQ(s, a): Q((2, 2), (3, 2)): 15.341851364399945\n",
      "\t\tQ(s, a): Q((2, 2), (2, 3)): 13.441851364399945\n",
      "\t\tQ(s, a): Q((2, 2), (2, 1)): 11.42689960516395\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 4), (4, 4)): 21.045063599999946\n",
      "\t\tQ(s, a): Q((3, 4), (2, 4)): 16.046501515999942\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 6), (7, 6)): 35.63999999999995\n",
      "\t\tQ(s, a): Q((8, 6), (8, 7)): 43.99999999999997\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 1), (4, 2)): 17.046501515999942\n",
      "\t\tQ(s, a): Q((4, 1), (3, 1)): 13.807666227959947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 1), (1, 2)): 12.42689960516395\n",
      "\t\tQ(s, a): Q((1, 1), (2, 1)): 11.42689960516395\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((0, 1), (1, 1)): 10.184209644647552\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 2), (4, 2)): 17.046501515999942\n",
      "\t\tQ(s, a): Q((3, 2), (3, 1)): 13.807666227959947\n",
      "\t\tQ(s, a): Q((3, 2), (2, 2)): 13.807666227959947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 6), (2, 7)): 17.384457237695823\n",
      "\t\tQ(s, a): Q((2, 6), (2, 5)): 15.845457239497868\n",
      "\t\tQ(s, a): Q((2, 6), (1, 6)): 14.845456780137878\n",
      "\t\tQ(s, a): Q((2, 6), (3, 6)): 20.67340399999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 2), (8, 1)): 15.064967239999891\n",
      "\t\tQ(s, a): Q((8, 2), (8, 3)): 20.944403999999945\n",
      "\t\tQ(s, a): Q((8, 2), (7, 2)): 19.944403999999903\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 5), (4, 4)): 21.045063599999946\n",
      "\t\tQ(s, a): Q((4, 5), (5, 5)): 25.981559999999945\n",
      "\t\tQ(s, a): Q((4, 5), (4, 6)): 24.081559999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 4), (1, 5)): 14.260911497821613\n",
      "\t\tQ(s, a): Q((1, 4), (2, 4)): 16.046501515999942\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 5), (7, 4)): 26.96839999999995\n",
      "\t\tQ(s, a): Q((7, 5), (7, 6)): 35.63999999999995\n",
      "\t\tQ(s, a): Q((7, 5), (6, 5)): 28.868399999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 3), (2, 4)): 16.046501515999942\n",
      "\t\tQ(s, a): Q((2, 3), (2, 2)): 13.807666227959947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 7), (8, 6)): 39.59999999999996\n",
      "\t\tQ(s, a): Q((8, 7), (8, 8)): 39.59999999999996\n",
      "\t\tQ(s, a): Q((8, 7), (7, 7)): 38.59999999999996\n",
      "\t\tQ(s, a): Q((8, 7), (9, 7)): 49.999999999999986\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 4), (6, 4)): 25.981559999999945\n",
      "\t\tQ(s, a): Q((5, 4), (4, 4)): 21.045063599999946\n",
      "\t\tQ(s, a): Q((5, 4), (5, 5)): 25.981559999999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 2), (5, 2)): 18.13055723999994\n",
      "\t\tQ(s, a): Q((4, 2), (3, 2)): 15.341851364399945\n",
      "\t\tQ(s, a): Q((4, 2), (4, 1)): 15.341851364399945\n",
      "\t\tQ(s, a): Q((4, 2), (4, 3)): 18.940557239999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 4), (6, 3)): 22.383403999999945\n",
      "\t\tQ(s, a): Q((6, 4), (7, 4)): 26.96839999999995\n",
      "\t\tQ(s, a): Q((6, 4), (5, 4)): 23.383403999999945\n",
      "\t\tQ(s, a): Q((6, 4), (6, 5)): 28.868399999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 5), (6, 4)): 25.981559999999945\n",
      "\t\tQ(s, a): Q((6, 5), (7, 5)): 31.07599999999995\n",
      "\t\tQ(s, a): Q((6, 5), (5, 5)): 25.981559999999945\n",
      "\t\tQ(s, a): Q((6, 5), (6, 6)): 32.075999999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 8), (7, 8)): 34.63999999999995\n",
      "\t\tQ(s, a): Q((6, 8), (5, 8)): 26.158399999999947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 7), (3, 7)): 19.316063599999925\n",
      "\t\tQ(s, a): Q((2, 7), (2, 6)): 17.606061573449296\n",
      "\t\tQ(s, a): Q((2, 7), (2, 8)): 18.06944509520112\n",
      "\t\tQ(s, a): Q((2, 7), (1, 7)): 15.645793187297457\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 3), (7, 3)): 23.271559999999948\n",
      "\t\tQ(s, a): Q((8, 3), (8, 2)): 17.849963599999946\n",
      "\t\tQ(s, a): Q((8, 3), (8, 4)): 23.271559999999948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 6), (5, 6)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((4, 6), (4, 5)): 22.383403999999945\n",
      "\t\tQ(s, a): Q((4, 6), (4, 7)): 22.573403999999943\n",
      "\t\tQ(s, a): Q((4, 6), (3, 6)): 20.67340399999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 8), (8, 8)): 39.59999999999996\n",
      "\t\tQ(s, a): Q((7, 8), (6, 8)): 30.17599999999995\n",
      "\t\tQ(s, a): Q((7, 8), (7, 7)): 38.59999999999996\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 1), (6, 2)): 20.145063599999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 1), (3, 2)): 15.341851364399945\n",
      "\t\tQ(s, a): Q((3, 1), (4, 1)): 15.341851364399945\n",
      "\t\tQ(s, a): Q((3, 1), (2, 1)): 11.42689960516395\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 4), (7, 3)): 23.271559999999948\n",
      "\t\tQ(s, a): Q((7, 4), (6, 4)): 25.981559999999945\n",
      "\t\tQ(s, a): Q((7, 4), (7, 5)): 31.07599999999995\n",
      "\t\tQ(s, a): Q((7, 4), (8, 4)): 23.271559999999948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 8), (7, 8)): 34.63999999999995\n",
      "\t\tQ(s, a): Q((8, 8), (8, 7)): 43.99999999999997\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 3), (4, 2)): 17.046501515999942\n",
      "\t\tQ(s, a): Q((4, 3), (4, 4)): 21.045063599999946\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 7), (2, 7)): 17.38445498783043\n",
      "\t\tQ(s, a): Q((1, 7), (1, 6)): 14.845053127119108\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 2), (4, 2)): 17.046501515999942\n",
      "\t\tQ(s, a): Q((5, 2), (6, 2)): 20.145063599999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 4), (3, 4)): 18.940557239999944\n",
      "\t\tQ(s, a): Q((2, 4), (2, 5)): 15.845457239999945\n",
      "\t\tQ(s, a): Q((2, 4), (2, 3)): 13.441851364399945\n",
      "\t\tQ(s, a): Q((2, 4), (1, 4)): 14.441851364399945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 4), (7, 4)): 26.96839999999995\n",
      "\t\tQ(s, a): Q((8, 4), (8, 3)): 20.944403999999945\n"
     ]
    }
   ],
   "source": [
    "learning_count = 1000\n",
    "QL_solver = QLearning_Solver(maze_field, display=True)\n",
    "for i in range(learning_count):\n",
    "    QL_solver.qlearn()\n",
    "\n",
    "QL_solver.dump_Qvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: [0, 1] -> action: [1, 1] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  @@   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 1 \t score: -1.0\n",
      "\n",
      "current state: [1, 1] -> action: [1, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1  @@   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 2 \t score: -1.0\n",
      "\n",
      "current state: [1, 2] -> action: [2, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1  @@  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 3 \t score: -1.0\n",
      "\n",
      "current state: [2, 2] -> action: [3, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0  @@   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 4 \t score: -1.0\n",
      "\n",
      "current state: [3, 2] -> action: [4, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0  @@   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 5 \t score: -1.0\n",
      "\n",
      "current state: [4, 2] -> action: [4, 3] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0  @@   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 6 \t score: -1.0\n",
      "\n",
      "current state: [4, 3] -> action: [4, 4] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0  @@  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 7 \t score: -1.0\n",
      "\n",
      "current state: [4, 4] -> action: [5, 4] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #  @@   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 8 \t score: -1.0\n",
      "\n",
      "current state: [5, 4] -> action: [5, 5] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0  @@  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 9 \t score: -1.0\n",
      "\n",
      "current state: [5, 5] -> action: [6, 5] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0  @@   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 10 \t score: -1.0\n",
      "\n",
      "current state: [6, 5] -> action: [6, 6] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0  @@   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 11 \t score: -1.0\n",
      "\n",
      "current state: [6, 6] -> action: [7, 6] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1  @@  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 12 \t score: -1.0\n",
      "\n",
      "current state: [7, 6] -> action: [8, 6] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #  @@  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 13 \t score: -1.0\n",
      "\n",
      "current state: [8, 6] -> action: [8, 7] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  @@   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 14 \t score: -2.0\n",
      "\n",
      "current state: [8, 7] -> action: [9, 7] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  @@   #   # \n",
      "current step: 15 \t score: 48.0\n",
      "\n",
      "Goal!\n"
     ]
    }
   ],
   "source": [
    "QL_solver.qlearn(greedy_flg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Solving the maze in Deep Q-learning\n",
    "### https://deepmind.com/research/dqn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN_Solver:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.e_decay = 0.9999\n",
    "        self.e_min = 0.01\n",
    "        self.learning_rate = 0.0001\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(2,2), activation='tanh'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember_memory(self, state, action, reward, next_state, next_movables, done):\n",
    "        self.memory.append((state, action, reward, next_state, next_movables, done))\n",
    "\n",
    "    def choose_action(self, state, movables):\n",
    "        if self.epsilon >= random.random():\n",
    "            return random.choice(movables)\n",
    "        else:\n",
    "            return self.choose_best_action(state, movables)\n",
    "        \n",
    "    def choose_best_action(self, state, movables):\n",
    "        best_actions = []\n",
    "        max_act_value = -100\n",
    "        for a in movables:\n",
    "            np_action = np.array([[state, a]])\n",
    "            act_value = self.model.predict(np_action)\n",
    "            if act_value > max_act_value:\n",
    "                best_actions = [a,]\n",
    "                max_act_value = act_value\n",
    "            elif act_value == max_act_value:\n",
    "                best_actions.append(a)\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def replay_experience(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, next_movables, done = minibatch[i]\n",
    "            input_action = [state, action]\n",
    "            if done:\n",
    "                target_f = reward\n",
    "            else:\n",
    "                next_rewards = []\n",
    "                for i in next_movables:\n",
    "                    np_next_s_a = np.array([[next_state, i]])\n",
    "                    next_rewards.append(self.model.predict(np_next_s_a))\n",
    "                np_n_r_max = np.amax(np.array(next_rewards))\n",
    "                target_f = reward + self.gamma * np_n_r_max\n",
    "            X.append(input_action)\n",
    "            Y.append(target_f)\n",
    "        np_X = np.array(X)\n",
    "        np_Y = np.array([Y]).T\n",
    "        self.model.fit(np_X, np_Y, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.e_min:\n",
    "            self.epsilon *= self.e_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/20000, score: 5.0, e: 1.0 \t @ 100\n",
      "episode: 500/20000, score: 19.0, e: 0.95 \t @ 86\n",
      "episode: 1000/20000, score: 14.0, e: 0.9 \t @ 82\n",
      "episode: 1500/20000, score: 24.0, e: 0.86 \t @ 56\n",
      "episode: 2000/20000, score: -9.0, e: 0.82 \t @ 138\n",
      "episode: 2500/20000, score: 26.0, e: 0.78 \t @ 76\n",
      "episode: 3000/20000, score: 20.0, e: 0.74 \t @ 84\n",
      "episode: 3500/20000, score: 42.0, e: 0.7 \t @ 24\n",
      "episode: 4000/20000, score: 44.0, e: 0.67 \t @ 22\n",
      "episode: 4500/20000, score: 45.0, e: 0.64 \t @ 24\n",
      "episode: 5000/20000, score: 46.0, e: 0.61 \t @ 36\n",
      "episode: 5500/20000, score: 33.0, e: 0.58 \t @ 32\n",
      "episode: 6000/20000, score: 43.0, e: 0.55 \t @ 32\n",
      "episode: 6500/20000, score: 48.0, e: 0.52 \t @ 24\n",
      "episode: 7000/20000, score: 38.0, e: 0.5 \t @ 34\n",
      "episode: 7500/20000, score: 45.0, e: 0.47 \t @ 28\n",
      "episode: 8000/20000, score: 43.0, e: 0.45 \t @ 42\n",
      "episode: 8500/20000, score: 43.0, e: 0.43 \t @ 24\n",
      "episode: 9000/20000, score: 45.0, e: 0.41 \t @ 28\n",
      "episode: 9500/20000, score: 45.0, e: 0.39 \t @ 28\n",
      "episode: 10000/20000, score: 48.0, e: 0.37 \t @ 20\n",
      "episode: 10500/20000, score: 44.0, e: 0.35 \t @ 22\n",
      "episode: 11000/20000, score: 46.0, e: 0.33 \t @ 24\n",
      "episode: 11500/20000, score: 45.0, e: 0.32 \t @ 22\n",
      "episode: 12000/20000, score: 48.0, e: 0.3 \t @ 26\n",
      "episode: 12500/20000, score: 45.0, e: 0.29 \t @ 18\n",
      "episode: 13000/20000, score: 47.0, e: 0.27 \t @ 18\n",
      "episode: 13500/20000, score: 41.0, e: 0.26 \t @ 24\n",
      "episode: 14000/20000, score: 47.0, e: 0.25 \t @ 16\n",
      "episode: 14500/20000, score: 47.0, e: 0.23 \t @ 14\n",
      "episode: 15000/20000, score: 47.0, e: 0.22 \t @ 14\n",
      "episode: 15500/20000, score: 48.0, e: 0.21 \t @ 14\n",
      "episode: 16000/20000, score: 46.0, e: 0.2 \t @ 18\n",
      "episode: 16500/20000, score: 44.0, e: 0.19 \t @ 20\n",
      "episode: 17000/20000, score: 48.0, e: 0.18 \t @ 14\n",
      "episode: 17500/20000, score: 45.0, e: 0.17 \t @ 20\n",
      "episode: 18000/20000, score: 48.0, e: 0.17 \t @ 18\n",
      "episode: 18500/20000, score: 48.0, e: 0.16 \t @ 20\n",
      "episode: 19000/20000, score: 46.0, e: 0.15 \t @ 18\n",
      "episode: 19500/20000, score: 48.0, e: 0.14 \t @ 16\n"
     ]
    }
   ],
   "source": [
    "state_size = 2\n",
    "action_size = 2\n",
    "dql_solver = DQN_Solver(state_size, action_size)\n",
    "\n",
    "episodes = 20000\n",
    "times = 1000\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = start_point\n",
    "    score = 0\n",
    "    for time in range(times):\n",
    "        movables = maze_field.get_actions(state)\n",
    "        action = dql_solver.choose_action(state, movables)\n",
    "        reward, done = maze_field.get_val(action)\n",
    "        score = score + reward\n",
    "        next_state = action\n",
    "        next_movables = maze_field.get_actions(next_state)\n",
    "        dql_solver.remember_memory(state, action, reward, next_state, next_movables, done)\n",
    "        if done or time == (times - 1):\n",
    "            if e % 500 == 0:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2} \\t @ {}\"\n",
    "                        .format(e, episodes, score, dql_solver.epsilon, time))\n",
    "            break\n",
    "        state = next_state\n",
    "    dql_solver.replay_experience(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: [0, 1] -> action: [1, 1] \n",
      "\t  #  @@   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 1 \t score: -1.0\n",
      "\n",
      "current state: [1, 1] -> action: [1, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  @@   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 2 \t score: -1.0\n",
      "\n",
      "current state: [1, 2] -> action: [2, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1  @@   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 3 \t score: -1.0\n",
      "\n",
      "current state: [2, 2] -> action: [3, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1  @@  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 4 \t score: -1.0\n",
      "\n",
      "current state: [3, 2] -> action: [4, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0  @@   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 5 \t score: -1.0\n",
      "\n",
      "current state: [4, 2] -> action: [4, 3] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0  @@   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 6 \t score: -1.0\n",
      "\n",
      "current state: [4, 3] -> action: [4, 4] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0  @@   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 7 \t score: -1.0\n",
      "\n",
      "current state: [4, 4] -> action: [5, 4] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0  @@  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 8 \t score: -1.0\n",
      "\n",
      "current state: [5, 4] -> action: [5, 5] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #  @@   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 9 \t score: -1.0\n",
      "\n",
      "current state: [5, 5] -> action: [6, 5] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0  @@  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 10 \t score: -1.0\n",
      "\n",
      "current state: [6, 5] -> action: [6, 6] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0  @@   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 11 \t score: -1.0\n",
      "\n",
      "current state: [6, 6] -> action: [7, 6] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0  @@   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 12 \t score: -1.0\n",
      "\n",
      "current state: [7, 6] -> action: [8, 6] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1  @@  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 13 \t score: -1.0\n",
      "\n",
      "current state: [8, 6] -> action: [8, 7] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #  @@  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 14 \t score: -2.0\n",
      "\n",
      "current state: [8, 7] -> action: [9, 7] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  @@   0   # \n",
      "\t  #   #   #   #   #   #   #  50   #   # \n",
      "current step: 15 \t score: 48.0\n",
      "\n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   0   #   0   0  -1   0   #   # \n",
      "\t  #  -1   0  -1  -1   0  -1   0  -1   # \n",
      "\t  #   0   0   #   0   #  -1  -1   0   # \n",
      "\t  #   0   0   0   0  -1  -1   0   0   # \n",
      "\t  #   #   0   #   0   0  -1   0  -1   # \n",
      "\t  #  -1   0  -1   0   0   0   #  -1   # \n",
      "\t  #   #  -1  -1  -1  -1   0  -1  -1   # \n",
      "\t  #  -1  -1   0  -1   #   0  -1   0   # \n",
      "\t  #   #   #   #   #   #   #  @@   #   # \n",
      "goal!\n"
     ]
    }
   ],
   "source": [
    "state = start_point\n",
    "score = 0\n",
    "steps = 0\n",
    "while True:\n",
    "    steps += 1\n",
    "    movables = maze_field.get_actions(state)\n",
    "    action = dql_solver.choose_best_action(state, movables)\n",
    "    print(\"current state: {0} -> action: {1} \".format(state, action))\n",
    "    reward, done = maze_field.get_val(action)\n",
    "    maze_field.display(state)\n",
    "    score = score + reward\n",
    "    state = action\n",
    "    print(\"current step: {0} \\t score: {1}\\n\".format(steps, score))\n",
    "    if done:\n",
    "        maze_field.display(action)\n",
    "        print(\"goal!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
